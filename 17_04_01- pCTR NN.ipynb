{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections as col\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097738"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train= pd.read_csv('dataset/train.csv', sep=',')\n",
    "X_train= X_train[:1097738]\n",
    "df_train= X_train\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299749"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid= pd.read_csv('dataset/validation.csv', sep=',')\n",
    "X_valid= pd.read_csv('dataset/validation.csv', sep=',')\n",
    "len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_day(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.weekday,prefix='day')],axis=1)\n",
    "    X = X.drop('weekday',axis=1)\n",
    "    return X\n",
    "\n",
    "# 2. Encode hours\n",
    "def enc_hrs(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.hour,prefix='hour')],axis=1)\n",
    "    X = X.drop('hour',axis=1)\n",
    "    return X\n",
    "\n",
    "# Split user agent into 2 ~ OS and browser\n",
    "def enc_OS_browser(X):\n",
    "    df = pd.DataFrame(X.useragent.str.split('_',1).tolist(),\n",
    "                                   columns = ['OS','browser'])\n",
    "    X = pd.concat([X,df],axis=1)\n",
    "\n",
    "    # 3. Encode OS\n",
    "    X = pd.concat([X,pd.get_dummies(X.OS,prefix='OS')],axis=1)\n",
    "    X = X.drop('OS',axis=1)\n",
    "\n",
    "    # 4. Encode browser\n",
    "    X = pd.concat([X,pd.get_dummies(X.browser,prefix='browser')],axis=1)\n",
    "    X = X.drop('browser',axis=1)\n",
    "    \n",
    "    X = X.drop('useragent',axis=1)\n",
    "    return X\n",
    "\n",
    "# 5. Encode region\n",
    "def enc_region(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.region,prefix='region')],axis=1)\n",
    "    X = X.drop('region',axis=1)\n",
    "    return X\n",
    "\n",
    "# 6. Encode adexchange\n",
    "def enc_adexchange(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.adexchange,prefix='adexchange')],axis=1)\n",
    "    X = X.drop('adexchange',axis=1)\n",
    "    return X\n",
    "\n",
    "# 7. Encode slotwidth\n",
    "def enc_slotwidth(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.slotwidth,prefix='slotwidth')],axis=1)\n",
    "    X = X.drop('slotwidth',axis=1)\n",
    "    return X\n",
    "\n",
    "# 8. Encode slotheight\n",
    "def enc_slotheight(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.slotheight,prefix='slotheight')],axis=1)\n",
    "    X = X.drop('slotheight',axis=1)\n",
    "    return X\n",
    "\n",
    "# 9. Encode slotvisibility\n",
    "def enc_slotvisibility(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.slotvisibility,prefix='slotvisibility')],axis=1)\n",
    "    X = X.drop('slotvisibility',axis=1)\n",
    "    return X\n",
    "\n",
    "# 10. Encode slotformat\n",
    "def enc_slotformat(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.slotformat,prefix='slotformat')],axis=1)\n",
    "    X = X.drop('slotformat',axis=1)\n",
    "    return X\n",
    "\n",
    "# 11. Encode advertiser\n",
    "def enc_advertiser(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.advertiser,prefix='advertiser')],axis=1)\n",
    "    X = X.drop('advertiser',axis=1)\n",
    "    return X\n",
    "\n",
    "# 12. Encoding slotprice into buckets\n",
    "def enc_slotprice(X):\n",
    "    bins = pd.DataFrame()\n",
    "    bins['slotprice_bins'] = pd.cut(X.slotprice.values,5, labels=[1,2,3,4,5])\n",
    "\n",
    "    X = pd.concat([X,bins],axis=1)\n",
    "    X = pd.concat([X,pd.get_dummies(X.slotprice_bins,prefix='slotprice')],axis=1)\n",
    "\n",
    "    X = X.drop('slotprice',axis=1)\n",
    "    X = X.drop('slotprice_bins',axis=1)\n",
    "    bins.pop('slotprice_bins')\n",
    "    return X\n",
    "\n",
    "# 13. Encoding user tags\n",
    "def enc_usertag(X):\n",
    "    a = pd.DataFrame(X.usertag.str.split(',').tolist())\n",
    "    usertag_df = pd.DataFrame(a)\n",
    "    usertag_df2 = pd.get_dummies(usertag_df,prefix='usertag')\n",
    "    usertag_df2 = usertag_df2.groupby(usertag_df2.columns, axis=1).sum()\n",
    "    X = pd.concat([X, usertag_df2], axis=1)\n",
    "    X = X.drop('usertag', axis=1)\n",
    "    return X\n",
    "\n",
    "# 14. Encoding cities\n",
    "def enc_city(X):\n",
    "    X = pd.concat([X,pd.get_dummies(X.city,prefix='city')],axis=1)\n",
    "    X = X.drop('city',axis=1)\n",
    "    return X\n",
    "\n",
    "def encode_labels(X):\n",
    "    X = enc_day(X)\n",
    "    X = enc_hrs(X)\n",
    "    X = enc_OS_browser(X)\n",
    "    X = enc_region(X)\n",
    "    X = enc_adexchange(X)\n",
    "    X = enc_slotwidth(X)\n",
    "    X = enc_slotheight(X)\n",
    "    X = enc_slotvisibility(X)\n",
    "    X = enc_slotformat(X)\n",
    "    X = enc_advertiser(X)\n",
    "#    X = enc_city(X)    # Don't encode cities\n",
    "    X = enc_slotprice(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_labels(X_train)\n",
    "X_train = enc_usertag(X_train)\n",
    "Y_train= pd.DataFrame(X_train[['click']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= X_train.drop(['click', 'bidid', 'logtype', 'userid', 'IP', 'city', 'domain',\n",
    "       'url', 'urlid', 'slotid', 'creative', 'bidprice', 'payprice',\n",
    "       'keypage'], axis= 1)\n",
    "Y_train_s = np.asarray(Y_train)[:, 0]\n",
    "Y_train_s= Y_train_s.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Y_train_s = pd.DataFrame(Y_train_s)\n",
    "df_Y_train_s['1'] = np.where(df_Y_train_s[0]==1, 0, 1) \n",
    "Y_train_s_2 = df_Y_train_s.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valid= encode_labels(X_valid)\n",
    "X_valid= enc_usertag(X_valid)\n",
    "Y_valid= pd.DataFrame(X_valid[['click']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid= X_valid.drop(['click', 'bidid', 'logtype', 'userid', 'IP', 'city', 'domain',\n",
    "       'url', 'urlid', 'slotid', 'creative', 'bidprice', 'payprice',\n",
    "       'keypage'], axis= 1)\n",
    "\n",
    "Y_valid_s = np.asarray(Y_valid)[:, 0]\n",
    "Y_valid_s= Y_valid_s.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Y_valid_s = pd.DataFrame(Y_valid_s)\n",
    "df_Y_valid_s['1'] = np.where(df_Y_valid_s[0]==1, 0, 1) \n",
    "Y_valid_s_2 = df_Y_valid_s.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NB pCTR and AUC\n",
    "def Naive_Bayes(array_x, array_y):\n",
    "    NB_model = GaussianNB()\n",
    "    NB_model.fit(array_x, array_y)\n",
    "    return NB_model\n",
    "\n",
    "def AUC_accuracy(XX_model, array_x, array_y):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(array_y, XX_model.predict_proba(array_x)[:, 1])\n",
    "    return metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pCTR_NB = pd.read_csv('val_NB_pCTR_1.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((299749, 1), array([[ 0.00027471],\n",
       "        [ 0.00035606],\n",
       "        [ 0.00030166],\n",
       "        ..., \n",
       "        [ 0.00023384],\n",
       "        [ 0.0004417 ],\n",
       "        [ 0.00028951]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(Y_valid).shape, np.reshape(df_pCTR_NB.prob_1.values, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504052394635\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(np.asarray(Y_valid), np.reshape(df_pCTR_NB.prob_1.values, (-1, 1)))\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesshields/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "NB_model_all = Naive_Bayes(np.asarray(X_train), np.asarray(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520481533189\n"
     ]
    }
   ],
   "source": [
    "print(AUC_accuracy(NB_model_all, np.asarray(X_valid), np.asarray(Y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_balance(X_data, Y_data_s_2, iter_):\n",
    "    \n",
    "    sample_sz= 10000\n",
    "    index_bline = iter_* sample_sz\n",
    "\n",
    "    nclick_index = np.random.randint(sample_sz, size=10)\n",
    "    click_index = np.where(Y_train_s_2[(iter_* sample_sz): ((iter_+ 1) *sample_sz), 1]== 0)\n",
    "    click_pos_r1 = np.random.randint(len(click_index[0]), size=1)\n",
    "    click_pos_r2 = np.random.randint(len(click_index[0]), size=1)\n",
    "    click_pos_r3 = np.random.randint(len(click_index[0]), size=1)\n",
    "    click_index_r = np.append(click_index[0][click_pos_r1], click_index[0][click_pos_r2]) #, click_index[0][click_pos_r3])\n",
    "\n",
    "    Y_click_r = Y_data_s_2[index_bline+ click_index_r]\n",
    "    X_click_r = np.asarray(X_data)[index_bline+ click_index_r]\n",
    "    Y_nclick_r = Y_data_s_2[index_bline+ nclick_index]\n",
    "    X_nclick_r = np.asarray(X_data)[index_bline+ nclick_index]\n",
    "\n",
    "    a = np.append(Y_click_r, Y_nclick_r, axis= 0)\n",
    "    b = np.append(X_click_r, X_nclick_r, axis= 0)\n",
    "    p = np.random.permutation(len(a))\n",
    "    \n",
    "    return b[p], a[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class pCTR_MLP():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.self = self\n",
    "\n",
    "    def initilise_model(self):\n",
    "        self.x_i = tf.placeholder(\"float\", [None, 219])\n",
    "        self.y_i = tf.placeholder(\"float\", [None, 2])\n",
    "\n",
    "        d_in = 219\n",
    "        d_hidden1 = 500\n",
    "        d_hidden2 = 500\n",
    "        d_out = 2\n",
    "        \n",
    "        self.W1 = tf.Variable(tf.random_normal([d_in, d_hidden1], mean= 0.01, stddev= 0.01))\n",
    "        self.b1 = tf.Variable(tf.random_normal([d_hidden1], mean= -2, stddev= 0.01))\n",
    "        self.W2 = tf.Variable(tf.random_normal([d_hidden1, d_hidden2], mean= 0.01, stddev= 0.01))\n",
    "        self.b2 = tf.Variable(tf.random_normal([d_hidden2], mean= -2, stddev= 0.01))\n",
    "        self.W3 = tf.Variable(tf.random_normal([d_hidden2, d_out], mean= 0.01, stddev= 0.01))\n",
    "        self.b3 = tf.Variable(tf.random_normal([d_out], mean= -0.5, stddev= 0.01))\n",
    "\n",
    "        self.a1_i = tf.matmul(self.x_i, self.W1)+ self.b1\n",
    "        self.z1_i = tf.sigmoid(self.a1_i)\n",
    "        self.a2_i = tf.matmul(self.z1_i, self.W2)+ self.b2\n",
    "        self.z2_i = tf.sigmoid(self.a2_i)\n",
    "        self.a3_i = tf.matmul(self.z2_i, self.W3)+ self.b3\n",
    "        self.y_hat = tf.nn.softmax(self.a3_i)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= self.y_i, logits=self.y_hat))\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.starter_learning_rate = 0.001\n",
    "        self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate, self.global_step,\n",
    "                                                   100000, 0.96, staircase=True)\n",
    "        self.optimiser = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss) #self.learning_rate\n",
    "        \n",
    "        self.prediction = tf.round(self.y_hat)\n",
    "        self.mistakes = tf.not_equal(self.y_i, self.prediction)\n",
    "        self.accuracy = 1- tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def show_var_init(self, x_array, y_array, iter_= 10):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.show_dict= {self.x_i: x_array[:1000],\n",
    "                         self.y_i: y_array[:1000]} \n",
    "        print('Pre op:', sess.run(self.y_hat, feed_dict= self.show_dict))\n",
    "        for i in range(iter_):\n",
    "            sess.run(self.optimiser, feed_dict= self.show_dict)\n",
    "        print('Post op:', sess.run(self.y_hat, feed_dict= self.show_dict))\n",
    "        \n",
    "    def show_var(self, x_val, y_val):\n",
    "        print('y_hat:', sess.run(self.y_hat, feed_dict= self.iter_dict))\n",
    "        print('y_i:', sess.run(self.y_i, feed_dict= self.iter_dict))\n",
    "        print('loss:', sess.run(self.loss, feed_dict= self.iter_dict))\n",
    "        print('accuracy:', sess.run(self.accuracy, feed_dict= self.iter_dict))\n",
    "        \n",
    "        x_array_b, y_array_b = batch_balance(x_val, y_val, 0)\n",
    "        self.valid_dict= {self.x_i: x_array_b,\n",
    "                          self.y_i: y_array_b} \n",
    "        print('accuracy:', sess.run(self.accuracy, feed_dict= self.valid_dict))\n",
    "\n",
    "    def train_full(self, x_array, y_array, epoch, batch_sz):\n",
    "\n",
    "        self.iter_= int(x_array.shape[0]/ batch_sz)\n",
    "        print('Iters: %s. Epoch:  %s' % (self.iter_, epoch))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.train_dict= {self.x_i: x_array,\n",
    "                          self.y_i: y_array} \n",
    "        \n",
    "        if self.iter_ % (self.iter_/10)== 0:\n",
    "            print(sess.run(self.loss, feed_dict= train_dict))\n",
    "        \n",
    "        for e in range(epoch):\n",
    "            e_loss= 0\n",
    "                \n",
    "            for i in range(self.iter_):\n",
    "                iter_dict= {self.x_i: x_array[(i* batch_sz):((i+ 1)* batch_sz)],\n",
    "                            self.y_i: y_array[(i* batch_sz):((i+ 1)* batch_sz)]}\n",
    "                \n",
    "                sess.run(self.optimiser, feed_dict= iter_dict)\n",
    "                e_loss+= sess.run(self.loss, feed_dict= iter_dict)\n",
    "                \n",
    "                if e% 1000== 0:\n",
    "                    print('Epoch %s, loss %s' % (e, sess.run(self.loss, feed_dict= iter_dict)))\n",
    "            \n",
    "    def train_full_bal(self, x_array, y_array, epoch, batch_sz):\n",
    "\n",
    "        self.iter_= int(x_array.shape[0]/ 10000)\n",
    "        #print('Iters: %s. Epoch:  %s' % (self.iter_, epoch))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.train_dict= {self.x_i: x_array,\n",
    "                          self.y_i: y_array} \n",
    "        \n",
    "        if self.iter_ % (self.iter_/10)== 0:\n",
    "            print(sess.run(self.loss, feed_dict= self.train_dict))\n",
    "        \n",
    "        for e in range(epoch):\n",
    "            e_loss= 0\n",
    "                \n",
    "            for i in range(self.iter_):\n",
    "                \n",
    "                x_array_b, y_array_b = batch_balance(x_array, y_array, i)\n",
    "                self.x_array_b = x_array_b\n",
    "                self.y_array_b = y_array_b\n",
    "                \n",
    "                self.iter_dict= {self.x_i: x_array_b,\n",
    "                                 self.y_i: y_array_b}\n",
    "                \n",
    "                sess.run(self.optimiser, feed_dict= self.iter_dict)\n",
    "                e_loss+= sess.run(self.loss, feed_dict= self.iter_dict)\n",
    "                #print('Epoch %s, iter %s, loss %s' % (e, self.iter_ ,sess.run(self.loss, feed_dict= iter_dict)))\n",
    "                #print(sess.run(self.y_i, feed_dict= self.iter_dict))\n",
    "                \n",
    "            if e% 20== 0:\n",
    "                pass\n",
    "                #print('Epoch %s, iter %s, loss %s' % (e, self.iter_ ,sess.run(self.loss, feed_dict= self.iter_dict)))\n",
    "            \n",
    "    def predict(self, x_array, y_array, save_run= False):\n",
    "        pred_dict= {self.x_i: x_array,\n",
    "                    self.y_i: y_array} \n",
    "\n",
    "        predict_proba = sess.run(self.y_hat, feed_dict= pred_dict)\n",
    "        self.df_predict_proba = pd.DataFrame(predict_proba)\n",
    "\n",
    "        if save_run == True:\n",
    "            output_directory = '/pCRT/'\n",
    "            output_filename = 'NN2_pCTR_validation.csv'\n",
    "            df_predict_proba.to_csv('NN2_pCTR_validation.csv', index= False)\n",
    "            print('pCTR file saved:', os.getcwd(), output_directory, output_filename)\n",
    "            \n",
    "    def test_accuracy(self, df_valid, sess, test_train):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve([click for click in df_valid.click.values], self.df_predict_proba[0].values)\n",
    "        print('AUC accuracy:', metrics.auc(fpr, tpr))\n",
    "        if (metrics.auc(fpr, tpr) > 0.74) & (metrics.auc(fpr, tpr) < 0.76):\n",
    "            self.df_predict_proba.to_csv('NB_pCTR_valid.csv', index= False)\n",
    "            print('SAVE')\n",
    "        if metrics.auc(fpr, tpr) > 0.79 :\n",
    "            self.df_predict_proba.to_csv('NN2_pCTR_valid.csv', index= False)\n",
    "            print('SAVE')\n",
    "        \n",
    "        #print('Pointwise accuracy', sess.run(self.accuracy, feed_dict= self.train_dict))\n",
    "        if test_train== 'test':\n",
    "            if metrics.auc(fpr, tpr)> 0.829294389288:\n",
    "                print('LR accuracy beat!')\n",
    "            #if metrics.auc(fpr, tpr)> 0.776775567252:\n",
    "                #self.save_model(sess)\n",
    "            \n",
    "    def save_model(self, sess):\n",
    "        print('Saving model...')\n",
    "        if not os.path.exists('./pCTR_NNmodel/'):\n",
    "            os.mkdir('./pCTR_NNmodel/')\n",
    "        self.saver.save(sess, './pCTR_NNmodel/model.checkpoint')\n",
    "        print('Model saved')\n",
    "\n",
    "    def load_model(self, sess):\n",
    "        print('Loading model...')\n",
    "        self.saver.restore(sess, './pCTR_NNmodel/model.checkpoint')\n",
    "        print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation:\n",
      "AUC accuracy: 0.597983699687\n",
      "Training:\n",
      "50 validation:\n",
      "AUC accuracy: 0.773283990572\n",
      "50 train:\n",
      "AUC accuracy: 0.705094918916\n",
      "100 validation:\n",
      "AUC accuracy: 0.780844160209\n",
      "100 train:\n",
      "AUC accuracy: 0.739477084934\n",
      "150 validation:\n",
      "AUC accuracy: 0.780652424671\n",
      "150 train:\n",
      "AUC accuracy: 0.759809073006\n",
      "SAVE\n",
      "200 validation:\n",
      "AUC accuracy: 0.776125337221\n",
      "200 train:\n",
      "AUC accuracy: 0.751619759148\n",
      "SAVE\n",
      "250 validation:\n",
      "AUC accuracy: 0.775148791889\n",
      "250 train:\n",
      "AUC accuracy: 0.737410622559\n",
      "300 validation:\n",
      "AUC accuracy: 0.775705399609\n",
      "300 train:\n",
      "AUC accuracy: 0.795630697858\n",
      "SAVE\n",
      "350 validation:\n",
      "AUC accuracy: 0.768952501734\n",
      "350 train:\n",
      "AUC accuracy: 0.674137722947\n",
      "400 validation:\n",
      "AUC accuracy: 0.773029544705\n",
      "400 train:\n",
      "AUC accuracy: 0.710034411603\n",
      "450 validation:\n",
      "AUC accuracy: 0.782496573682\n",
      "450 train:\n",
      "AUC accuracy: 0.79071622644\n",
      "SAVE\n",
      "500 validation:\n",
      "AUC accuracy: 0.78036450375\n",
      "500 train:\n",
      "AUC accuracy: 0.771111326981\n"
     ]
    }
   ],
   "source": [
    "model = pCTR_MLP()\n",
    "model.initilise_model()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.set_random_seed(3)\n",
    "    #model.load_model(sess)\n",
    "    \n",
    "    print('Initialisation:')\n",
    "    model.train_full_bal(np.asarray(X_train), Y_train_s_2, 1, 10)\n",
    "    model.predict(np.asarray(X_valid), Y_valid_s_2)\n",
    "    model.test_accuracy(df_valid, sess, 'test')\n",
    "    \n",
    "    print('Training:')\n",
    "    it_count= 0\n",
    "    for iteration in [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]: #, 50, 50, 50, 50]: #, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]:\n",
    "        \n",
    "        it_count+= iteration\n",
    "        print(it_count, 'validation:')\n",
    "        model.train_full_bal(np.asarray(X_train), Y_train_s_2, iteration, 10)\n",
    "        model.predict(np.asarray(X_valid), Y_valid_s_2)\n",
    "        model.test_accuracy(df_valid, sess, 'test')\n",
    "        \n",
    "        print(it_count, 'train:')\n",
    "        model.predict(np.asarray(X_train[:20000]), Y_valid_s_2[:20000])\n",
    "        model.test_accuracy(df_train[:20000], sess, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(np.asarray(X_train), Y_train_s_2[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a= NB_model.predict_proba(np.asarray(X_valid))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC accuracy: 0.479518466811\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve([click for click in df_valid.click.values], NB_model.predict_proba(np.asarray(X_valid))[:, 1])\n",
    "print('AUC accuracy:', metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.click.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.08844942e-082,   2.01208014e-139,   1.79012609e-085, ...,\n",
       "         1.01377636e-077,   6.27794160e-073,   9.06464692e-085])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_model.predict_proba(np.asarray(X_valid))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = pd.DataFrame()\n",
    "b['df_valid'] = df_valid.click.values\n",
    "b['predict_proba'] = NB_model.predict_proba(np.asarray(X_valid))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'float' and 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-29cd37ada9cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'float' and 'bool'"
     ]
    }
   ],
   "source": [
    "b.loc[b['predict_proba'] > 0.1 & (1==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
